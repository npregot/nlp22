{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "0dcd9295-b8f8-407d-a93e-ee6e5033f96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll use to make predictions for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn0XtfFeqP2P",
        "outputId": "643edf49-0b48-4f75-a219-0e19dd6f88ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-02 08:18:39--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.26M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-02-02 08:18:40 (50.3 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2022-02-02 08:18:40--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-02 08:18:40 (53.8 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2022-02-02 08:18:40--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   6.27M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-02-02 08:18:41 (167 MB/s) - ‘test.txt’ saved [6573426/6573426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "*First*, let's define a classifier based on a really simple dictionary-based feature: if the abstract contains the words \"love\" or \"like\", the CONTAINS_POSITIVE_WORD feature will fire, and if it contains either \"hate\" or \"dislike\", the CONTAINS_NEGATIVE_WORD will fire.  Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.  Note the `L2_regularization_strength` specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and the `min_feature_count` specifies how many data points need to contain a feature for it to be allowable as a feature in the model.  Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnqjxd6fKPiP",
        "outputId": "a2fe1587-a017-4d6b-866a-79701b557695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "First, is this accuracy score any good?  Let's calculate the accuracy of a majority class predictor to provide some context.  Again, this determines the most represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t--LfOjPj7T",
        "outputId": "4172a9b0-f80c-4076-e157-3323a4d4a9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "# Your assignment\n",
        "\n",
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm three additional distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data. \n",
        "\n",
        "Describe your features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words||\n",
        "|Feature 1||\n",
        "|Feature 2||\n",
        "|Feature 3||\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# MY HELPER FUNCTIONS #\n",
        "#######################\n",
        "'''\n",
        "Reads a text file, and appends the words into a list\n",
        "'''\n",
        "def file_to_list(filename):\n",
        "\n",
        "\tmy_file = open(filename, \"r\")\n",
        "\tcontent_list = my_file.readlines()\n",
        "\ti=0 \n",
        "\twhile i < len(content_list):\n",
        "\t\tcontent_list[i] = content_list[i][:len(content_list[i])-1]\n",
        "\t\ti+=1\n",
        "\treturn content_list\n",
        "\n",
        "'''\n",
        "Removes special chars except '-', for hyphens words like: 'eighteenth-century'\n",
        "'''\n",
        "def remove_special_chars(word):\n",
        "\tword_arr = []\n",
        "\tfor ch in word:\n",
        "\t\tif ch == '-':\n",
        "\t\t\tword_arr.append(ch)\n",
        "\t\telif 65 <= ord(ch) <= 90 or 97 <= ord(ch) <= 122:\n",
        "\t\t\t\tword_arr.append(ch)\n",
        "\toutput = ''\n",
        "\treturn output.join(word_arr)\n",
        " \n",
        "''' \n",
        "Finds the top nth negative or positive terms,\n",
        "and returns them as a list\n",
        "'''\n",
        "def top_terms(text, filedata, n=20):\n",
        "\n",
        "  arr = file_to_list(filedata)\n",
        "  tk_text = nltk.word_tokenize(text)\n",
        "  top = []\n",
        "  dic_freq = {}\n",
        "  for i in arr:\n",
        "    dic_freq[i] = 0\n",
        "  \n",
        "  # removing special chars from text\n",
        "  for term in tk_text:\n",
        "    term = remove_special_chars(term.lower())\n",
        "  \n",
        "  # building the dictionary frequency\n",
        "  for word in tk_text:\n",
        "    if word in dic_freq:\n",
        "      dic_freq[word] +=1\n",
        "\n",
        "  # sorting dictionary\n",
        "  dic_freq = dict(sorted(dic_freq.items(), key=lambda item: item[1], reverse=True))\n",
        "  \n",
        "  # appending the top values\n",
        "  for k,v in dic_freq.items():\n",
        "    if len(top) == n:\n",
        "      break\n",
        "    else:\n",
        "      top.append(k)\n",
        "\n",
        "  return top"
      ],
      "metadata": {
        "id": "pw4_eRKH0aaf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    feats = {}\n",
        "    for word in nltk.word_tokenize(text):\n",
        "      word = remove_special_chars(word.lower())\n",
        "      if word not in feats:\n",
        "        feats[word] = 1\n",
        "                        \n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AJ5qMBeqmL",
        "outputId": "836e7f87-474a-4ba2-9d12-a96f6bf0796b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 20326, Train accuracy: 1.000, Dev accuracy: 0.765\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For feature1, I am looking for positive words from a large list. I found the list here: https://gist.github.com/mkulakowski2/4289437\n",
        "\n",
        "1. I read the file 'positive.txt' and store the words into a list\n",
        "\n",
        "2. I tokenize the text, and I also made my own helper function called \"remove_special_chars\" to remove punctuation, so that \"Terrible!\" -> \"terrible\"\n",
        "\n",
        "3. I parse the tokenized text \"tk_text\" and see if there is any positive term. If there is, I add 1 to the dictionary \"feats\".\n",
        "\n",
        "Additionally, I am trying to handle two edge scenarios: \n",
        "\n",
        "(i) i.e.\"did not like\", or \"did not love\". In this case, the counting of positive terms, shouldn't go up. \n",
        "\n",
        "(ii) \"it was good, but...\", in this scenario, my tokenized text \"tk_text\" will be 'it','was','good','but'; therefore, if after a positive word, there is 'but', it shouldn't count\n",
        "\n",
        "\n",
        "In conclusion, I believe this feature is valid because it searches for positive terms, and these positive terms come from a large list."
      ],
      "metadata": {
        "id": "03Xpmulga1Mj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "# POSITIVE [check if it's preceded by \"not\"]\n",
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    feats = {'has_positive': 0}\n",
        "    positive_arr = file_to_list('positive.txt')\n",
        "    tk_text = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in tk_text:\n",
        "      word = remove_special_chars(word.lower())\n",
        "\n",
        "    i=0\n",
        "    while i < len(tk_text):\n",
        "      if remove_special_chars(tk_text[i].lower()) in positive_arr:\n",
        "        if remove_special_chars(tk_text[i-1].lower()) != 'not':\n",
        "          if i == len(tk_text) - 1:\n",
        "            feats['has_positive'] += 1\n",
        "          else:\n",
        "            if remove_special_chars(tk_text[i+1].lower()) != 'but' or remove_special_chars(tk_text[i+1].lower()) != 'except':\n",
        "              feats['has_positive'] += 1\n",
        "      i+=1\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eg6LvrjL9eRW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MAwRwbQ7lVw",
        "outputId": "47af35bb-d59e-4a7c-8987-ccbf22e69293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 1, Train accuracy: 0.602, Dev accuracy: 0.622\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For feature2, I am looking for negative terms. I applied the same logic as in feature1. \n",
        "I found the list here: https://gist.github.com/mkulakowski2/4289441\n",
        "\n",
        "Similarly to feature1, I believe feature2 is valid too because it searches for negative terms, and these negative terms come from a large list. "
      ],
      "metadata": {
        "id": "746dRQhKduF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "# NEGATIVE\n",
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {'has_negative': 0}\n",
        "    negative_arr = file_to_list('negative.txt')\n",
        "    tk_text = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in tk_text:\n",
        "      word = remove_special_chars(word.lower())\n",
        "\n",
        "    i=0\n",
        "    while i < len(tk_text):\n",
        "      if remove_special_chars(tk_text[i].lower()) in negative_arr:\n",
        "        if remove_special_chars(tk_text[i-1].lower()) != 'not':\n",
        "          if i == len(tk_text) - 1:\n",
        "            feats['has_negative'] += 1\n",
        "          else:\n",
        "            if remove_special_chars(tk_text[i+1].lower()) != 'but' or remove_special_chars(tk_text[i+1].lower()) != 'except':\n",
        "              feats['has_negative'] += 1\n",
        "      i+=1\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgpuykF67oWZ",
        "outputId": "6e1e3789-ab76-4290-cb08-e382b8d10766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 1, Train accuracy: 0.599, Dev accuracy: 0.576\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For feature3, the main difference is that I'm first running my helper function called \"top_terms\" to collect the top negative terms. You can find \"top_terms\" along with my other helper functions in the cell that is before \"bag_of_words\"\n",
        "\n",
        "\"top_terms()\" works this way:\n",
        "\n",
        "1. For every negative/positive term, it initializes a frequency dictionary like this {term: 0, term2: 0, etc...}  \n",
        "2. For every word in \"text\", if it matches any key in the dictionary, its count will increase\n",
        "3. I'm sorting the dictionary, and returning the top nth terms as a list (by default the function returns the top 20).\n",
        "\n",
        "The rest of the logic is identical to features1-2. Overall, I believe this is a valid feature because rather than simply taking a large list of positive/negative terms, I'm first seeing which ones appear more frequently in the text, and from that, I create my dataset from where I take my positive/negative words that I compare with each review."
      ],
      "metadata": {
        "id": "iAo7bHO8oqvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    feats = {'has_negative': 0}\n",
        "    negative_arr = top_terms(text, 'negative.txt')\n",
        "    tk_text = nltk.word_tokenize(text)\n",
        "    for word in tk_text:\n",
        "      word = remove_special_chars(word.lower())\n",
        "\n",
        "    i=0\n",
        "    while i < len(tk_text):\n",
        "      if remove_special_chars(tk_text[i].lower()) in negative_arr:\n",
        "        if remove_special_chars(tk_text[i-1].lower()) != 'not':\n",
        "          if i == len(tk_text) - 1:\n",
        "            feats['has_negative'] += 1\n",
        "          else:\n",
        "            if remove_special_chars(tk_text[i+1].lower()) != 'but' or remove_special_chars(tk_text[i+1].lower()) != 'except':\n",
        "              feats['has_negative'] += 1\n",
        "      i+=1\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_f--utb7q4l",
        "outputId": "49ae278a-6a79-4d0e-8798-d9e600e1eff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 1, Train accuracy: 0.606, Dev accuracy: 0.589\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-tRUFTIdAqT",
        "outputId": "9c5495aa-9a82-48ed-b7e0-b640059b2b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 20328, Train accuracy: 1.000, Dev accuracy: 0.783\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Deliverable 2\n",
        "\n",
        "This code will generate a file named `combiner_function_predictions.csv`; download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 systems with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "## Interrogating classifiers\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  **Note that nothing below this line requires any work on your part; treat these as useful tools for understanding what works and what doesn't.**\n",
        "\n",
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels).  What kinds of mistakes is it making?  (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "7ulxd1TosIMV",
        "outputId": "ecda31d1-437f-4e72-eae8-6f8a074a7820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most defining for each of the classes (ranked by how strong their corresponding coefficient is).  Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyGuXIi9pqe",
        "outputId": "ead2d1a3-cf4f-4f10-857e-78e359537fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.748\tvery\n",
            "pos\t0.606\tgreat\n",
            "pos\t0.562\texcellent\n",
            "pos\t0.531\tseen\n",
            "pos\t0.525\ttop\n",
            "pos\t0.510\tenjoyed\n",
            "pos\t0.498\tviolence\n",
            "pos\t0.493\tsaw\n",
            "pos\t0.487\tsimple\n",
            "pos\t0.479\trelationship\n",
            "pos\t0.468\tthought\n",
            "pos\t0.466\tback\n",
            "pos\t0.463\taway\n",
            "pos\t0.452\tfriends\n",
            "pos\t0.446\tnew\n",
            "pos\t0.431\tman\n",
            "pos\t0.430\tdefinitely\n",
            "pos\t0.414\tamazing\n",
            "pos\t0.407\ttrue\n",
            "pos\t0.404\tperformance\n",
            "pos\t0.399\tyou\n",
            "pos\t0.393\tseries\n",
            "pos\t0.387\tin\n",
            "pos\t0.386\thim\n",
            "pos\t0.386\tperfect\n",
            "\n",
            "neg\t-0.856\tworst\n",
            "neg\t-0.816\tacting\n",
            "neg\t-0.720\tactors\n",
            "neg\t-0.666\twatching\n",
            "neg\t-0.663\tidea\n",
            "neg\t-0.634\twould\n",
            "neg\t-0.616\tboring\n",
            "neg\t-0.594\tnothing\n",
            "neg\t-0.549\twere\n",
            "neg\t-0.544\tscript\n",
            "neg\t-0.523\tthere\n",
            "neg\t-0.497\tstupid\n",
            "neg\t-0.496\tmaybe\n",
            "neg\t-0.483\tleast\n",
            "neg\t-0.483\tinstead\n",
            "neg\t-0.481\tmoney\n",
            "neg\t-0.478\tcan\n",
            "neg\t-0.468\tcould\n",
            "neg\t-0.457\twork\n",
            "neg\t-0.452\tsaid\n",
            "neg\t-0.446\twaste\n",
            "neg\t-0.445\tsilly\n",
            "neg\t-0.439\tenough\n",
            "neg\t-0.436\tsomething\n",
            "neg\t-0.424\teven\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are most mistaken. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UXmRhSuzxaJi",
        "outputId": "a4e5801e-bd47-4d66-8b4b-61082ddbba39"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-60832c66-bee7-49ce-b2f4-705a7a39f9f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1436</td>\n",
              "      <td>0.999968</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1822</td>\n",
              "      <td>0.999955</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1551</td>\n",
              "      <td>0.999937</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I firmly believe that the best Oscar ceremony in recent years was in 2003 for two reasons: 1 ) Host Steve Martin was at his most wittiest: \" I saw the teamsters help Michael Moore into the trunk of his limo \" and \" I'll better not mention the gay mafia in case I wake up with a poodle's head in my bed \" 2 ) Surprise winners: No one had Adrien Brody down for best actor ( Genuine applause ) or Ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>0.999767</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1247</td>\n",
              "      <td>0.999273</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1035</td>\n",
              "      <td>0.999022</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Emma is my favourite Jane Austen novel - Emma is well-meaning despite her flaws, so readers can forgive and love her, and the relationship she has with Mr Knightley, which is warm, familiar, respectful but playful, generating that warm, fuzzy, romantic excitement. Mr Knightley is the perfect man, and Emma is as close as you could get in those times to an independent, clever, confident woman - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1659</td>\n",
              "      <td>0.998469</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1677</td>\n",
              "      <td>0.998281</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Hello. this is my first review for any movie i have seen. i went through the trouble of doing this to tell everyone that this is quite literally, the most disgusting movie i have ever seen. I feel like the movie was porely made, which i will give some understanding due to budget constraints on making it. I felt like i was watching a very bad remake of the movie saw. Which i can agree, saw as w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1833</td>\n",
              "      <td>0.998062</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1576</td>\n",
              "      <td>0.997976</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Billy Chung Siu Hung's (the bloody swordplay film Assassin from 1993) film Love To Kill (Hong Kong, 1993) is among the strongest products of the Category III boom that inhabited the HK cinema in early nineties. It consisted of films with strong sex, nudity and violence, more or less gratuitous and shock valued only. Love To Kill definitely belongs to the \"more\" category with some unforgettable...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1010</td>\n",
              "      <td>0.997160</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Okay, you have:Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She's killed off in the first scene - that's right, folks; this show has no backbone!Peter O'Toole as Ol' Colonel Cricket from The First War and now the emblazered Lord of the Manor.Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1456</td>\n",
              "      <td>0.993106</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immoral teens should have focused more on the forbidden romance and why this was... should have really gotten into it instead of scraping the surface with basically \"because mom says we can't.\" Some parts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1787</td>\n",
              "      <td>0.992980</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1916</td>\n",
              "      <td>0.992836</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Having read the other comments on this film, I would like to share my own view that this is one tough movie to see unless you are a total Brooksophile. I am not.When looked at by a purely objective observer, the film is an unbalanced narrative that presents us with more undistilled neuroses than are capable of being absorbed in one sitting. It is quite difficult to watch. The Brooks character ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1178</td>\n",
              "      <td>0.992162</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1511</td>\n",
              "      <td>0.989765</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Wracked with guilt after a lot of things felt apart on that ledge, an ace mountain rescue climber Gabriel Walker (Stallone) comes back for his girlfriend Jessie (Janine Turner), while over the cloudy skies where the weather looks a bit threatening, a spectacularly precarious mid-air hijacking goes wrong and $100 million taken from a Treasury Department plane get lost in the middle of nowhere f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1309</td>\n",
              "      <td>0.988505</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1493</td>\n",
              "      <td>0.987927</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Yes, this was pure unbelievable condescending babble. We know that the French often have a skewed idea of the USA, it's puritanism and views towards sex. As an American (Hoosier) who lives in France, I have ample opportunity to observe these attitudes. And while some of these preconceived notions may be true, NOT ONE ELEMENT of the midwestern town portrayed in this film rang real. A man who ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1312</td>\n",
              "      <td>0.986031</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>All of the reviews here about how much ZP lacks plot, the acting is wooden, the orgy scene makes no sense, etc., all miss the main point.Let's be honest. This is a movie made in the heady times of late 1960s and early 1970s Los Angeles. It is a movie meant to be watched while your are H-I-G-H out of your mind on some psychedelic substance.Find some kind bud and smoke up, or get a mild hit of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1825</td>\n",
              "      <td>0.985623</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the way through or of their own free will. The thing about Kerching! is that it's a children's show, FOR CHILDREN so obviously if your older it is going to seem cheesy, forced, and probably stupid. I even...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60832c66-bee7-49ce-b2f4-705a7a39f9f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60832c66-bee7-49ce-b2f4-705a7a39f9f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60832c66-bee7-49ce-b2f4-705a7a39f9f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      id  ...                                                                                                                                                                                                                                                                                                                                                                                                             Text\n",
              "0   1436  ...  I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...\n",
              "1   1822  ...  Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...\n",
              "2   1551  ...  I firmly believe that the best Oscar ceremony in recent years was in 2003 for two reasons: 1 ) Host Steve Martin was at his most wittiest: \" I saw the teamsters help Michael Moore into the trunk of his limo \" and \" I'll better not mention the gay mafia in case I wake up with a poodle's head in my bed \" 2 ) Surprise winners: No one had Adrien Brody down for best actor ( Genuine applause ) or Ro...\n",
              "3   1004  ...  Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...\n",
              "4   1247  ...  Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...\n",
              "5   1035  ...  Emma is my favourite Jane Austen novel - Emma is well-meaning despite her flaws, so readers can forgive and love her, and the relationship she has with Mr Knightley, which is warm, familiar, respectful but playful, generating that warm, fuzzy, romantic excitement. Mr Knightley is the perfect man, and Emma is as close as you could get in those times to an independent, clever, confident woman - ...\n",
              "6   1659  ...  Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...\n",
              "7   1677  ...  Hello. this is my first review for any movie i have seen. i went through the trouble of doing this to tell everyone that this is quite literally, the most disgusting movie i have ever seen. I feel like the movie was porely made, which i will give some understanding due to budget constraints on making it. I felt like i was watching a very bad remake of the movie saw. Which i can agree, saw as w...\n",
              "8   1833  ...  This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...\n",
              "9   1576  ...  Billy Chung Siu Hung's (the bloody swordplay film Assassin from 1993) film Love To Kill (Hong Kong, 1993) is among the strongest products of the Category III boom that inhabited the HK cinema in early nineties. It consisted of films with strong sex, nudity and violence, more or less gratuitous and shock valued only. Love To Kill definitely belongs to the \"more\" category with some unforgettable...\n",
              "10  1010  ...  Okay, you have:Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She's killed off in the first scene - that's right, folks; this show has no backbone!Peter O'Toole as Ol' Colonel Cricket from The First War and now the emblazered Lord of the Manor.Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still...\n",
              "11  1456  ...  Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immoral teens should have focused more on the forbidden romance and why this was... should have really gotten into it instead of scraping the surface with basically \"because mom says we can't.\" Some parts...\n",
              "12  1787  ...  My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her more after the documentary. I felt awful watching this documentary, it was like reliving the nightmare and it still brings tears to my eyes.But I'm extremely grateful that I watched this documentary,...\n",
              "13  1916  ...  Having read the other comments on this film, I would like to share my own view that this is one tough movie to see unless you are a total Brooksophile. I am not.When looked at by a purely objective observer, the film is an unbalanced narrative that presents us with more undistilled neuroses than are capable of being absorbed in one sitting. It is quite difficult to watch. The Brooks character ...\n",
              "14  1178  ...  While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...\n",
              "15  1511  ...  Wracked with guilt after a lot of things felt apart on that ledge, an ace mountain rescue climber Gabriel Walker (Stallone) comes back for his girlfriend Jessie (Janine Turner), while over the cloudy skies where the weather looks a bit threatening, a spectacularly precarious mid-air hijacking goes wrong and $100 million taken from a Treasury Department plane get lost in the middle of nowhere f...\n",
              "16  1309  ...  Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...\n",
              "17  1493  ...  Yes, this was pure unbelievable condescending babble. We know that the French often have a skewed idea of the USA, it's puritanism and views towards sex. As an American (Hoosier) who lives in France, I have ample opportunity to observe these attitudes. And while some of these preconceived notions may be true, NOT ONE ELEMENT of the midwestern town portrayed in this film rang real. A man who ha...\n",
              "18  1312  ...  All of the reviews here about how much ZP lacks plot, the acting is wooden, the orgy scene makes no sense, etc., all miss the main point.Let's be honest. This is a movie made in the heady times of late 1960s and early 1970s Los Angeles. It is a movie meant to be watched while your are H-I-G-H out of your mind on some psychedelic substance.Find some kind bud and smoke up, or get a mild hit of a...\n",
              "19  1825  ...  Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the way through or of their own free will. The thing about Kerching! is that it's a children's show, FOR CHILDREN so obviously if your older it is going to seem cheesy, forced, and probably stupid. I even...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "analyze(big_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxwwblfh9pqf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}