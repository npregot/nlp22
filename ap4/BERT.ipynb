{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npregot/nlp22/blob/main/ap4/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/AP/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYm2QjD4WPA"
      },
      "source": [
        "BERT for binary or multiclass document classification using the [CLS] token as the document representation; trains a model (on `train.txt`), uses `dev.txt` for early stopping, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals.\n",
        "\n",
        "Before executing this notebook on Colab, make sure you're running on cuda (`Runtime > Change runtime type > GPU`) to make use of GPU speedups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXwcKMY44a04"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeSgrDjF4WPC"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9qWkf0z4WPE"
      },
      "outputs": [],
      "source": [
        "# If you have your folder of data on your Google drive account, you can connect that here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcoE7WJQ6XRp"
      },
      "outputs": [],
      "source": [
        "# Change this to the directory with your data\n",
        "directory=\"/content/drive/MyDrive/ap_data/0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ycKOkx34WPE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuzjCRrG4WPF"
      },
      "outputs": [],
      "source": [
        "def read_labels(filename):\n",
        "    labels={}\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[1]\n",
        "            if label not in labels:\n",
        "                labels[label]=len(labels)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XetKMAw4WPF"
      },
      "outputs": [],
      "source": [
        "def read_data(filename, labels, max_data_points=1000):\n",
        "  \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[1]\n",
        "            text = cols[2]\n",
        "            \n",
        "            data.append(text)\n",
        "            data_labels.append(labels[label])\n",
        "            \n",
        "\n",
        "    # shuffle the data\n",
        "    tmp = list(zip(data, data_labels))\n",
        "    random.shuffle(tmp)\n",
        "    data, data_labels = zip(*tmp)\n",
        "    \n",
        "    if max_data_points is None:\n",
        "        return data, data_labels\n",
        "    \n",
        "    return data[:max_data_points], data_labels[:max_data_points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPfJqVmb4WPH"
      },
      "outputs": [],
      "source": [
        "labels=read_labels(\"%s/train.txt\" % directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-728X0b4WPI"
      },
      "outputs": [],
      "source": [
        "train_x, train_y=read_data(\"%s/train.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQOE469I4WPI"
      },
      "outputs": [],
      "source": [
        "dev_x, dev_y=read_data(\"%s/dev.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMpv6ZV54WPI"
      },
      "outputs": [],
      "source": [
        "test_x, test_y=read_data(\"%s/test.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUKRjWsf4WPJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, x, y):\n",
        "    model.eval()\n",
        "    corr = 0.\n",
        "    total = 0.\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(x, y):\n",
        "            y_preds=model.forward(x)\n",
        "            for idx, y_pred in enumerate(y_preds):\n",
        "                prediction=torch.argmax(y_pred)\n",
        "                if prediction == y[idx]:\n",
        "                    corr += 1.\n",
        "                total+=1                          \n",
        "    return corr/total, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkUSuG794WPK"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model_name, params):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.model_name=bert_model_name\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n",
        "        self.bert = BertModel.from_pretrained(self.model_name)\n",
        "        \n",
        "        self.num_labels = params[\"label_length\"]\n",
        "\n",
        "        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
        "\n",
        "    def get_batches(self, all_x, all_y, batch_size=32, max_toks=510):\n",
        "            \n",
        "        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n",
        "      (and limited to a maximum number of WordPiece tokens \"\"\"\n",
        "\n",
        "        batches_x=[]\n",
        "        batches_y=[]\n",
        "        \n",
        "        for i in range(0, len(all_x), batch_size):\n",
        "\n",
        "            current_batch=[]\n",
        "\n",
        "            x=all_x[i:i+batch_size]\n",
        "\n",
        "            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
        "            batch_y=all_y[i:i+batch_size]\n",
        "\n",
        "            batches_x.append(batch_x.to(device))\n",
        "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
        "            \n",
        "        return batches_x, batches_y\n",
        "  \n",
        "\n",
        "    def forward(self, batch_x): \n",
        "    \n",
        "        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
        "                         attention_mask=batch_x[\"attention_mask\"],\n",
        "                         token_type_ids=batch_x[\"token_type_ids\"],\n",
        "                         output_hidden_states=True)\n",
        "\n",
        "      # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n",
        "      # And use the *last* layer output (layer -1)\n",
        "      # as a result of this choice, this embedding will be optimized for this purpose during the training process.\n",
        "      \n",
        "        bert_hidden_states = bert_output['hidden_states']\n",
        "\n",
        "        out = bert_hidden_states[-1][:,0,:]\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mVrY9yv4WPL"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiiXWwz94WPL"
      },
      "outputs": [],
      "source": [
        "def train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None):\n",
        "\n",
        "    bert_model = BERTClassifier(bert_model_name, params={\"label_length\": len(labels), \"doLowerCase\":doLowerCase, \"embedding_size\":embedding_size})\n",
        "    bert_model.to(device)\n",
        "\n",
        "    batch_x, batch_y = bert_model.get_batches(train_x, train_y)\n",
        "    dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)\n",
        "\n",
        "    optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs=30\n",
        "    best_dev_acc = 0.\n",
        "    patience=5\n",
        "\n",
        "    best_epoch=0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        bert_model.train()\n",
        "\n",
        "        # Train\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            y_pred = bert_model.forward(x)\n",
        "            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        dev_accuracy, _=evaluate(bert_model, dev_batch_x, dev_batch_y)\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "            if dev_accuracy > best_dev_acc:\n",
        "                torch.save(bert_model.state_dict(), model_filename)\n",
        "                best_dev_acc = dev_accuracy\n",
        "                best_epoch=epoch\n",
        "        if epoch - best_epoch > patience:\n",
        "            print(\"No improvement in dev accuracy over %s epochs; stopping training\" % patience)\n",
        "            break\n",
        "\n",
        "    bert_model.load_state_dict(torch.load(model_filename))\n",
        "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
        "    return bert_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twbsysmd4WPM"
      },
      "outputs": [],
      "source": [
        "# small BERT -- can run on laptop\n",
        "# bert_model_name=\"google/bert_uncased_L-2_H-128_A-2\"\n",
        "# model_filename=\"mybert.model\"\n",
        "# embedding_size=128\n",
        "# doLowerCase=True\n",
        "\n",
        "# bert-base -- slow on laptop; better on Colab\n",
        "bert_model_name=\"bert-base-cased\"\n",
        "model_filename=\"mybert.model\"\n",
        "embedding_size=768\n",
        "doLowerCase=False\n",
        "\n",
        "model=train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=doLowerCase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyImsx_C4WPN"
      },
      "outputs": [],
      "source": [
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
        "accuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper=confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4wL2Z8w4WPO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}