{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npregot/nlp22/blob/main/HW6/HW6_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f0112f",
      "metadata": {
        "id": "54f0112f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/main/HW5/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4f183a",
      "metadata": {
        "id": "0a4f183a"
      },
      "source": [
        "## Homework 6\n",
        "\n",
        "In this homework, you will be working with WordNet synsets and exploring methods to align new words (not in WordNet) with an existing synset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0adaa568",
      "metadata": {
        "id": "0adaa568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd7395b-e74d-499b-be4e-b15a44b73724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "--2022-04-06 18:15:59--  https://people.ischool.berkeley.edu/~dbamman/glove.6B.100d.100K.txt\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85951834 (82M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.100K.txt.2’\n",
            "\n",
            "glove.6B.100d.100K. 100%[===================>]  81.97M  27.2MB/s    in 3.0s    \n",
            "\n",
            "2022-04-06 18:16:03 (27.2 MB/s) - ‘glove.6B.100d.100K.txt.2’ saved [85951834/85951834]\n",
            "\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.5.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.18.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.63.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "!wget https://people.ischool.berkeley.edu/~dbamman/glove.6B.100d.100K.txt\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c188261",
      "metadata": {
        "id": "9c188261"
      },
      "source": [
        "# Preliminaries: WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0cba00",
      "metadata": {
        "id": "dd0cba00"
      },
      "source": [
        "NLTK provides a great interface to the WordNet ontology.  Remember that core unit within WordNet is the **synset** (a category of near-synonyms).  A word (like \"blue\") can appear in many different synsets, each corresponding to a distinct *sense* of that word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "852cf585",
      "metadata": {
        "id": "852cf585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713dcc57-2c2f-4616-d959-11c14a594697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('blue.n.01') blue color or pigment; resembling the color of the clear sky in the daytime\n",
            "Synset('blue.n.02') blue clothing\n",
            "Synset('blue.n.03') any organization or party whose uniforms or badges are blue\n",
            "Synset('blue_sky.n.01') the sky as viewed during daylight\n",
            "Synset('bluing.n.01') used to whiten laundry or hair or give it a bluish tinge\n",
            "Synset('amobarbital_sodium.n.01') the sodium salt of amobarbital that is used as a barbiturate; used as a sedative and a hypnotic\n",
            "Synset('blue.n.07') any of numerous small butterflies of the family Lycaenidae\n",
            "Synset('blue.v.01') turn blue\n",
            "Synset('blue.s.01') of the color intermediate between green and violet; having a color similar to that of a clear unclouded sky\n",
            "Synset('blue.s.02') used to signify the Union forces in the American Civil War (who wore blue uniforms)\n",
            "Synset('gloomy.s.02') filled with melancholy and despondency\n",
            "Synset('blasphemous.s.02') characterized by profanity or cursing\n",
            "Synset('blue.s.05') suggestive of sexual impropriety\n",
            "Synset('aristocratic.s.01') belonging to or characteristic of the nobility or aristocracy\n",
            "Synset('blue.s.07') morally rigorous and strict\n",
            "Synset('blue.s.08') causing dejection\n"
          ]
        }
      ],
      "source": [
        "# get all of the synsets that a specific word belongs to; print their definitions\n",
        "\n",
        "synsets=wn.synsets('blue')\n",
        "for synset in synsets:\n",
        "    print (synset, synset.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f268c3",
      "metadata": {
        "id": "28f268c3"
      },
      "source": [
        "Any given synset will likewise contain multiple different words (all near-synonyms of each other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5f809838",
      "metadata": {
        "id": "5f809838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70806a27-0ca0-47af-da72-c35e9eb7c6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gloomy\n",
            "grim\n",
            "blue\n",
            "depressed\n",
            "dispirited\n",
            "down\n",
            "downcast\n",
            "downhearted\n",
            "down_in_the_mouth\n",
            "low\n",
            "low-spirited\n"
          ]
        }
      ],
      "source": [
        "# get all of the words/phrase in a given synset\n",
        "\n",
        "for lemma in wn.synset(\"gloomy.s.02\").lemmas():\n",
        "    print (lemma.name())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a2d1e5",
      "metadata": {
        "id": "a7a2d1e5"
      },
      "source": [
        "Remember also that one of the powerful things about WordNet is that it places synsets within a hierarchical structure; a given synset has both **hypernyms** (other synsets that it is a subclass of) and **hyponyms** (other synsets that are subclasses of it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "53b8ff2f",
      "metadata": {
        "id": "53b8ff2f"
      },
      "outputs": [],
      "source": [
        "# Functions from http://www.nltk.org/howto/wordnet.html to get *all* of a synset's hyponym/hypernyms\n",
        "\n",
        "hypo = lambda s: s.hyponyms()\n",
        "hyper = lambda s: s.hypernyms()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033786e8",
      "metadata": {
        "id": "033786e8"
      },
      "source": [
        "Find all of the synsets that are hyponyms of the target synset (descendents in the WordNet hierarchy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "15a5988f",
      "metadata": {
        "id": "15a5988f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8e2be4-bdfd-4ec8-be65-a9ae8548217a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('azure.n.01'),\n",
              " Synset('dark_blue.n.01'),\n",
              " Synset('greenish_blue.n.01'),\n",
              " Synset('powder_blue.n.01'),\n",
              " Synset('prussian_blue.n.02'),\n",
              " Synset('purplish_blue.n.01'),\n",
              " Synset('steel_blue.n.01'),\n",
              " Synset('ultramarine.n.02')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "list(wn.synset(\"blue.n.01\").closure(hypo))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b305eb",
      "metadata": {
        "id": "84b305eb"
      },
      "source": [
        "Find all of the synsets that are hyperyms (ancestors up the tree) of the target synset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2c22cece",
      "metadata": {
        "id": "2c22cece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8999479-a55c-41a0-9295-b9f9eb9726e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('chromatic_color.n.01'),\n",
              " Synset('color.n.01'),\n",
              " Synset('visual_property.n.01'),\n",
              " Synset('property.n.02'),\n",
              " Synset('attribute.n.02'),\n",
              " Synset('abstraction.n.06'),\n",
              " Synset('entity.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "list(wn.synset(\"blue.n.01\").closure(hyper))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac1001c",
      "metadata": {
        "id": "5ac1001c"
      },
      "source": [
        "Here's how you can access all of the synsets in WordNet through NLTK (though note executing this may take a while, so it's commented out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "449d8b45",
      "metadata": {
        "id": "449d8b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9fe1a0-1624-4ad7-fee6-a867276f3c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Synset('able.a.01')\n",
            "1 Synset('unable.a.01')\n",
            "2 Synset('abaxial.a.01')\n",
            "3 Synset('adaxial.a.01')\n",
            "4 Synset('acroscopic.a.01')\n",
            "5 Synset('basiscopic.a.01')\n",
            "6 Synset('abducent.a.01')\n",
            "7 Synset('adducent.a.01')\n",
            "8 Synset('nascent.a.01')\n",
            "9 Synset('emergent.s.02')\n",
            "10 Synset('dissilient.s.01')\n",
            "11 Synset('parturient.s.02')\n"
          ]
        }
      ],
      "source": [
        "for idx, synset in enumerate(wn.all_synsets()):\n",
        "   print(idx, synset)\n",
        "   if (idx > 10): break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b4cc005",
      "metadata": {
        "id": "0b4cc005"
      },
      "source": [
        "# Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cbe0f1a",
      "metadata": {
        "id": "3cbe0f1a"
      },
      "source": [
        "WordNet is a great resource, but one of its downsides is *coverage* -- many of the words in our vocabulay aren't in WordNet, but could conceivably be placed within existing synsets within it.  Your task for this homework is to develop two methods to finding the closest synset for a given new word from Urban Dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26301aaf",
      "metadata": {
        "id": "26301aaf"
      },
      "source": [
        "For the scope of this homework, we're only going to pretend that WordNet only has 12 different synsets within it (though feel free to use the `wn.all_synsets` function above if you wanted to explore running it on all of WordNet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2f0c3485",
      "metadata": {
        "id": "2f0c3485"
      },
      "outputs": [],
      "source": [
        "target_synsets=['spread.n.01', 'formidable.s.01', 'coziness.n.01', 'mutation.n.02', 'kernel.n.03', 'faineant.s.01', 'fund-raise.v.01', 'orientation.n.06', 'inappropriate.a.01', 'stranger.n.02', 'plausibility.n.01', 'sever.v.01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "81f2c025",
      "metadata": {
        "id": "81f2c025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea75d7d-b0bf-426b-a302-abd08ee115c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('spread.n.01')\n",
            "\tDefinition: process or result of distributing or extending over a wide expanse of space\n",
            "Synset('formidable.s.01')\n",
            "\tDefinition: extremely impressive in strength or excellence\n",
            "Synset('coziness.n.01')\n",
            "\tDefinition: a state of warm snug comfort\n",
            "Synset('mutation.n.02')\n",
            "\tDefinition: (genetics) any event that changes genetic structure; any alteration in the inherited nucleic acid sequence of the genotype of an organism\n",
            "Synset('kernel.n.03')\n",
            "\tDefinition: the choicest or most essential or most vital part of some idea or experience\n",
            "Synset('faineant.s.01')\n",
            "\tDefinition: disinclined to work or exertion\n",
            "Synset('fund-raise.v.01')\n",
            "\tDefinition: raise money for a cause or project\n",
            "Synset('orientation_course.n.01')\n",
            "\tDefinition: a course introducing a new situation or environment\n",
            "Synset('inappropriate.a.01')\n",
            "\tDefinition: not suitable for a particular occasion etc\n",
            "Synset('stranger.n.02')\n",
            "\tDefinition: an individual that one is not acquainted with\n",
            "Synset('plausibility.n.01')\n",
            "\tDefinition: apparent validity\n",
            "Synset('sever.v.01')\n",
            "\tDefinition: set or keep apart\n"
          ]
        }
      ],
      "source": [
        "for synset in target_synsets:\n",
        "    wn_synset=wn.synset(synset)\n",
        "    print(wn_synset)\n",
        "    print(\"\\tDefinition:\", wn_synset.definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff98e02",
      "metadata": {
        "id": "cff98e02"
      },
      "source": [
        "Here are the words that do not exist in WordNet now but that we want to add.  Each element of the tuple is (word, definition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "20eae7c7",
      "metadata": {
        "id": "20eae7c7"
      },
      "outputs": [],
      "source": [
        "urban_dictionary_terms: List[Tuple[str, str]] = [\n",
        "    (\"Crowdfunding\", \"the practice of obtaining needed funding (as for a new business) by soliciting contributions from a large number of people especially from the online community\"), \n",
        "    (\"Hygge\", \"a cozy quality that makes a person feel content and comfortable\"), \n",
        "    (\"biohacking\", \"biological experimentation (as by gene editing or the use of drugs or implants) done to improve the qualities or capabilities of living organisms especially by individuals and groups working outside a traditional medical or scientific research environment\"), \n",
        "    (\"TL;DR\", \"a briefly expressed main point or key message that summarizes a longer discussion or explanation\"), \n",
        "    (\"Hellacious\", \"Exceptionally powerful or violent; remarkably good; extremely difficult; extraordinarily large\"), \n",
        "    (\"Unfriend\", \"To remove from one's list of friends (e.g. on a social networking website)\"), \n",
        "    (\"Infodemic\", \"A wide and rapid spread of misinformation through various media, namely social media\"),\n",
        "    (\"Onboarding\", \"The act or process of orienting and training a new employee\"), \n",
        "    (\"Truthiness\", \"something that seems true but isn’t backed up by evidence\"), \n",
        "    (\"Amotivational\", \"Relating to, or characterised by, a lack of motivation\"), \n",
        "    (\"NSFW\", \"Not Safe For Work. Used to describe Internet content generally inappropriate for the typical workplace, i.e., would not be acceptable in the presence of your boss and colleagues\"),\n",
        "    (\"Rando\", \"a person who is not known or recognizable or whose appearance (as in a conversation or narrative) seems unprompted or unwelcome\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890ffff6",
      "metadata": {
        "id": "890ffff6"
      },
      "source": [
        "Your task here is to develop two different methods for finding the best matching synset.\n",
        "1. Find the WordNet synset with the highest cosine similarity between the average GloVe embeddings of its synset definition and the average GloVe embeddings of the new word definition.\n",
        "2. Find the WordNet synset with the highest cosine similarity between the sentence embedding its synset definition and the sentence embedding of the new word definition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afcfbfde",
      "metadata": {
        "id": "afcfbfde"
      },
      "source": [
        "Here is some code for reading in Glove embeddings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6204372b",
      "metadata": {
        "id": "6204372b"
      },
      "outputs": [],
      "source": [
        "def read_vectors(filename: str):\n",
        "    vocab_map={}\n",
        "    embeddings=[]\n",
        "    with(open(filename, encoding=\"utf-8\")) as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            cols=line.rstrip().split(\" \")\n",
        "            word=cols[0]\n",
        "            embedding=cols[1:]\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            vocab_map[word]=idx\n",
        "    \n",
        "    return vocab_map, np.array(embeddings, dtype=\"float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "90c21c48",
      "metadata": {
        "id": "90c21c48"
      },
      "outputs": [],
      "source": [
        "glove_vocab_map, glove_embeddings=read_vectors(\"glove.6B.100d.100K.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f398eaec",
      "metadata": {
        "id": "f398eaec"
      },
      "source": [
        "Here is some code for loading the sentence transformer package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "82797d7d",
      "metadata": {
        "id": "82797d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359f17f4-cd32-4949-f777-972401b712a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768,)\n"
          ]
        }
      ],
      "source": [
        "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
        "\n",
        "sentence_vector=sentence_model.encode(\"this is a sentence\")\n",
        "print(sentence_vector.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CEciw_iyhWZ3",
      "metadata": {
        "id": "CEciw_iyhWZ3"
      },
      "source": [
        "Here's an implementation of cosine similarity that you will find useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "uQU8eoHKhJ4h",
      "metadata": {
        "id": "uQU8eoHKhJ4h"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(one, two):\n",
        "    return np.dot(one, two) / (np.linalg.norm(one) * np.linalg.norm(two))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102ce6a9",
      "metadata": {
        "id": "102ce6a9"
      },
      "source": [
        "#### Q1. Implement the first method as `method_one` below.\n",
        "\n",
        "As mentioned above, you should compute the average GloVe embedding of the UD word definition and use cosine similarity to compare it with the average GloVe embedding of the synset definitions. For each UD word, choose the definition that maximizes the cosine similarity with its definition. Here are some things you need to do when calculating the average GloVe embedding of a sentence:\n",
        "- Use `nltk.word_tokenize()` to tokenize the sentence.\n",
        "- Treat everything as lowercase.\n",
        "- Skip any tokens which don't appear in the GloVe vocabulary.\n",
        "- Calculate the average value of the embedding vectors, which will be another vector of the same shape.\n",
        "\n",
        "Your function should return a dictionary mapping each urban dictionary term to a WordNet synset ID, e.g.:\n",
        "\n",
        "`{\n",
        " \"adore\" : \"love.v.01\",\n",
        " \"dripping\" : \"stylish.a.01\"    \n",
        " }`\n",
        "\n",
        " Please make sure that any helper functions that you use are defined *within* `method_one`! That will help us extract your code more easily with the autograder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5737fbe2",
      "metadata": {
        "id": "5737fbe2"
      },
      "outputs": [],
      "source": [
        "from pickle import NONE\n",
        "def method_one(urban_dictionary_terms: List[Tuple[str, str]], target_synsets: List[str]):\n",
        "    \"\"\"\n",
        "    Method 1: an algorithm based on GloVe embeddings that maps each urban dictionary term to a synset ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    urban_dictionary_terms : List[Tuple[str, str]]\n",
        "        a list of string 2-tuples where the first elements are words, second elements are definitions.\n",
        "    target_synsets : List[str]\n",
        "        a list of synset IDs that the words should be classified into.\n",
        "        You can call `wn.synset(\"<synset ID>\")` to get the synset object.\n",
        "    \n",
        "    Returns\n",
        "    --------\n",
        "    A dictionary mapping each urban dictionary term to a WordNet synet ID, e.g.\n",
        "    `{\"adore\" : \"love.v.01\", \"dripping\" : \"stylish.a.01\"}`\n",
        "    \n",
        "    \"\"\"\n",
        "    # Your code\n",
        "    '''\n",
        "    Returns the avg word embeddings of a sentence like \"I love food\"\n",
        "    returns: [-0.071953  0.23127   0.023731 ... n]\n",
        "    '''\n",
        "    def helper_one(sentence):\n",
        "      t = nltk.word_tokenize(sentence.lower())\n",
        "      matrix = []\n",
        "      for word in t:\n",
        "        # skip the word if not in the vocab_map\n",
        "        if glove_vocab_map.get(word) != None:\n",
        "          matrix.append(glove_embeddings[glove_vocab_map.get(word)])\n",
        "      return np.mean(matrix, axis=0)\n",
        "    \n",
        "    dic = {}\n",
        "    for x in urban_dictionary_terms:\n",
        "      token_definition = helper_one(x[1])\n",
        "      mx = float('-inf')\n",
        "      s = ''\n",
        "      for syn in target_synsets:\n",
        "        wn_synset = helper_one(wn.synset(syn).definition())\n",
        "        comparison = cosine_similarity(token_definition, wn_synset)\n",
        "        if comparison > mx:\n",
        "          mx = comparison\n",
        "          s = syn\n",
        "      dic[x[0]] = s\n",
        "    return dic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "b69452b2",
      "metadata": {
        "id": "b69452b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ce44f7-6911-405f-b395-0e368096b60b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Amotivational': 'kernel.n.03',\n",
              " 'Crowdfunding': 'spread.n.01',\n",
              " 'Hellacious': 'formidable.s.01',\n",
              " 'Hygge': 'stranger.n.02',\n",
              " 'Infodemic': 'spread.n.01',\n",
              " 'NSFW': 'stranger.n.02',\n",
              " 'Onboarding': 'orientation.n.06',\n",
              " 'Rando': 'stranger.n.02',\n",
              " 'TL;DR': 'stranger.n.02',\n",
              " 'Truthiness': 'stranger.n.02',\n",
              " 'Unfriend': 'stranger.n.02',\n",
              " 'biohacking': 'kernel.n.03'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "method_one_results=method_one(urban_dictionary_terms, target_synsets)\n",
        "method_one_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b862967",
      "metadata": {
        "id": "0b862967"
      },
      "source": [
        "#### Q2. Implement your second method as `method_two` below.\n",
        "\n",
        "In this function, you should compute the cosine similarity between the sentence embedding of the UD word definition and those of the synsets, then for each UD word, choose the synset with the highest cosine similarity. For consistency, use the sentence transformer model called `sentence-transformers/all-distilroberta-v1`.\n",
        "\n",
        "Your function must also return a dictionary mapping each urban dictionary term to a WordNet synset ID, e.g.:\n",
        "\n",
        "`{\n",
        " \"adore\" : \"love.v.01\",\n",
        " \"dripping\" : \"stylish.a.01\"    \n",
        " }`\n",
        "\n",
        "As before, please make sure that any helper functions that you use are defined *within* `method_two`! That will help us extract your code more easily with the autograder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "7d2b70a3",
      "metadata": {
        "id": "7d2b70a3"
      },
      "outputs": [],
      "source": [
        "def method_two(urban_dictionary_terms: List[Tuple[str, str]], target_synsets: List[str]):\n",
        "  \"\"\"\n",
        "  Method 2: an algorithm based on sentence embeddings that maps each urban dictionary term to a synset ID.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  urban_dictionary_terms : List[Tuple[str, str]]\n",
        "      a list of string 2-tuples where the first elements are words, second elements are definitions.\n",
        "  target_synsets : List[str]\n",
        "      a list of synset IDs that the words should be classified into.\n",
        "      You can call `wn.synset(\"<synset ID>\")` to get the synset object.\n",
        "  \n",
        "  Returns\n",
        "  --------\n",
        "  A dictionary mapping each urban dictionary term to a WordNet synet ID, e.g.\n",
        "  `{\"adore\" : \"love.v.01\", \"dripping\" : \"stylish.a.01\"}`\n",
        "  \n",
        "  \"\"\"\n",
        "  # Your code\n",
        "  dic = {}\n",
        "  for element in urban_dictionary_terms:\n",
        "    vectorized_ud_definition = sentence_model.encode(element[1])\n",
        "    mx = float('-inf')\n",
        "    s = ''\n",
        "    for syn in target_synsets:\n",
        "      vectorized_syn = sentence_model.encode(wn.synset(syn).definition())\n",
        "      comparison = cosine_similarity(vectorized_ud_definition, vectorized_syn)\n",
        "      if comparison > mx:\n",
        "        mx = comparison\n",
        "        s = syn\n",
        "    dic[element[0]] = s\n",
        "  return dic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "285b848a",
      "metadata": {
        "id": "285b848a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3250d7-55e3-4b22-ebcb-28379eb6fabc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Amotivational': 'faineant.s.01',\n",
              " 'Crowdfunding': 'fund-raise.v.01',\n",
              " 'Hellacious': 'formidable.s.01',\n",
              " 'Hygge': 'coziness.n.01',\n",
              " 'Infodemic': 'spread.n.01',\n",
              " 'NSFW': 'faineant.s.01',\n",
              " 'Onboarding': 'orientation.n.06',\n",
              " 'Rando': 'stranger.n.02',\n",
              " 'TL;DR': 'kernel.n.03',\n",
              " 'Truthiness': 'plausibility.n.01',\n",
              " 'Unfriend': 'stranger.n.02',\n",
              " 'biohacking': 'mutation.n.02'}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "method_two_results=method_two(urban_dictionary_terms, target_synsets)\n",
        "method_two_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f329391",
      "metadata": {
        "id": "5f329391"
      },
      "source": [
        "#### Q3: Define an evaluation metric (accuracy).  \n",
        "\n",
        "Throughout this semester we've stressed how critical evaluation is for any NLP method.  Implement a function `accuracy` that assesses quality of the dictionaries you return from `method_one` and `method_two`.  This accuracy function should return a single real number (the accuracy), and its input parameters are a prediction dict (the output of your model) and a truth dict (which you will need to create based on your own judgement). Make sure that the accuracies you calculate for the two methods match what you expect. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "938b9fcd",
      "metadata": {
        "id": "938b9fcd"
      },
      "outputs": [],
      "source": [
        "def accuracy(prediction: Dict[str, str], truth: Dict[str, str]) -> float:\n",
        "  if len(prediction) != len(truth):\n",
        "    return None\n",
        "  counter = 0\n",
        "  for i in prediction:\n",
        "    if prediction.get(i) == truth.get(i):\n",
        "      counter+=1\n",
        "  return counter / len(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "766150ed",
      "metadata": {
        "id": "766150ed"
      },
      "outputs": [],
      "source": [
        "truth = {'Amotivational': 'faineant.s.01', \n",
        " 'Crowdfunding': 'fund-raise.v.01',\n",
        " 'Hellacious': 'formidable.s.01',\n",
        " 'Hygge': 'coziness.n.01',\n",
        " 'Infodemic': 'spread.n.01', \n",
        " 'NSFW': 'faineant.s.01', \n",
        " 'Onboarding': 'orientation.n.06',\n",
        " 'Rando': 'stranger.n.02', \n",
        " 'TL;DR': 'spread.n.01', \n",
        " 'Truthiness': 'plausibility.n.01', \n",
        " 'Unfriend': 'stranger.n.02',\n",
        " 'biohacking': 'mutation.n.02'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9ea46c6b",
      "metadata": {
        "id": "9ea46c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fc3d42-3932-4224-bc35-8baa88093f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4166666666666667\n"
          ]
        }
      ],
      "source": [
        "print(accuracy(method_one_results, truth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "c0135985",
      "metadata": {
        "id": "c0135985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1299ce-1ddf-43d5-b61d-2d266725d7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9166666666666666\n"
          ]
        }
      ],
      "source": [
        "print(accuracy(method_two_results, truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eZv8eZJ8sfE",
      "metadata": {
        "id": "8eZv8eZJ8sfE"
      },
      "source": [
        "That concludes homework 6! To submit, just upload this .ipynb file to Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "djEZMMFT7j5G",
      "metadata": {
        "id": "djEZMMFT7j5G"
      },
      "source": [
        "#### Q4 (optional)\n",
        "\n",
        "Use the two methods you've defined to find the best-matching synset within the **entire** WordNet. Do the results make sense? Is one method consistently better than the other? Why?\n",
        "\n",
        "Here's a reminder of how to iterate through all synsets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "6M_gxGMo7g2T",
      "metadata": {
        "id": "6M_gxGMo7g2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5890e9e2-5c4d-4f78-8dc3-4b70b58e0853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Synset('able.a.01')\n",
            "1 Synset('unable.a.01')\n",
            "2 Synset('abaxial.a.01')\n",
            "3 Synset('adaxial.a.01')\n",
            "4 Synset('acroscopic.a.01')\n",
            "5 Synset('basiscopic.a.01')\n",
            "6 Synset('abducent.a.01')\n",
            "7 Synset('adducent.a.01')\n",
            "8 Synset('nascent.a.01')\n",
            "9 Synset('emergent.s.02')\n",
            "10 Synset('dissilient.s.01')\n",
            "11 Synset('parturient.s.02')\n"
          ]
        }
      ],
      "source": [
        "for idx, synset in enumerate(wn.all_synsets()):\n",
        "   print(idx, synset)\n",
        "   if (idx > 10): break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "z7Jpm8zN726O",
      "metadata": {
        "id": "z7Jpm8zN726O"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW6 student.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}